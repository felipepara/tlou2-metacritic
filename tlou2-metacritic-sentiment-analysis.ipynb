{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time as time\n",
    "import random as rand \n",
    "from IPython.display import clear_output\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "import nltk # imports the natural language toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict = {'date':[], 'rating':[], 'review':[]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents_list = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',\n",
    "                    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',\n",
    "                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36',\n",
    "                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "                    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "                    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    }
   ],
   "source": [
    "for page in range(0,367): #Remember to update the number of pages \n",
    "    url = 'https://www.metacritic.com/game/playstation-4/the-last-of-us-part-ii/user-reviews?sort-by=score&num_items=100&page='+str(page)\n",
    "    user_agent = {'User-agent': user_agents_list[rand.randint(0,len(user_agents_list)-1)]}\n",
    "    response  = requests.get(url, headers = user_agent)\n",
    "    time.sleep(1) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for review in soup.find_all('div', class_='review_content'):\n",
    "        \n",
    "        if review.find('div', class_='name') == None or review.find('div', class_='review_body').find('span') == None:\n",
    "                break\n",
    "        review_dict['date'].append(review.find('div', class_='date').text)\n",
    "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
    "        \n",
    "        if review.find('span', class_='blurb blurb_expanded'):\n",
    "            review_dict['review'].append(review.find('span', class_='blurb blurb_expanded').text)\n",
    "\n",
    "        else:\n",
    "            review_dict['review'].append(review.find('div', class_='review_body').find('span').text)\n",
    "\n",
    "    clear_output(wait = True)    \n",
    "    print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.DataFrame(review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('datasets/reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's treat the text properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['review'] = reviews['review'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "review_backup = reviews['review'].copy()\n",
    "reviews.review = reviews['review'].apply(lambda x: re.sub('[^A-Za-z0-9 ]+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.rating = reviews['rating'].astype(int)\n",
    "\n",
    "reviews_good = reviews[reviews['rating'] > 6]\n",
    "reviews_bad = reviews[reviews['rating'] < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "eng_stopwords = stopwords.words('english')\n",
    "### Getting a single string\n",
    "bad_reviews_text = ' '.join(reviews_bad.review)\n",
    "## Splitting them into tokens\n",
    "word_tokens = nltk.word_tokenize(bad_reviews_text)\n",
    "## Removing the stopwords\n",
    "word_tokens_clean = [each for each in word_tokens if each.lower() not in eng_stopwords and len(each.lower()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nesnasim gaye nesnasim gaye', 704), ('gaye nesnasim gaye nesnasim', 704), ('trash trash trash trash', 246), ('bad bad bad bad', 93), ('lgbt lgbt lgbt lgbt', 90), ('sucks sucks sucks sucks', 88), ('last abby part 2the', 82), ('abby part 2the last', 82), ('part 2the last abby', 82), ('2the last abby part', 82), ('disappointment immeasurable day ruined', 63), ('insult fansan insult fansan', 62), ('fansan insult fansan insult', 62), ('story story story story', 60), ('bad movie bad movie', 59), ('garbage garbage garbage garbage', 59), ('movie bad movie bad', 58), ('made first game great', 47), ('terrible terrible terrible terrible', 45), ('worst game ever played', 44), ('game game game game', 41), ('abby abby abby abby', 39), ('one best games ever', 36), ('ruined waited years play', 36), ('nani nani nani nani', 36), ('juego aburrido primero fue', 36), ('aburrido primero fue bueno', 36), ('por jugabilidad historia pero', 36), ('jugabilidad historia pero quedaron', 36), ('historia pero quedaron obsoletas', 36), ('pero quedaron obsoletas porque', 36), ('quedaron obsoletas porque cambiaron', 36), ('obsoletas porque cambiaron nada', 36), ('porque cambiaron nada solo', 36), ('cambiaron nada solo extiende', 36), ('nada solo extiende historia', 36), ('solo extiende historia manera', 36), ('extiende historia manera innecesaria', 36), ('immeasurable day ruined waited', 35), ('day ruined waited years', 35), ('expect poor story years', 35), ('poor story years waiting', 35), ('gay gay gay gay', 34), ('really expect poor story', 33), ('historia manera innecesaria mataron', 33), ('main character first game', 32), ('everything made first game', 32), ('hate game hate game', 32), ('stupid story telling bad', 32), ('story telling bad ending', 32)]\n"
     ]
    }
   ],
   "source": [
    "top_k_ngrams(word_tokens_clean,4,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11630    nothing but  average  is not capable for this ...\n",
       "11640    the gameplay is okay and the graphics are grea...\n",
       "11645    absolute disrespect towards the 1st game  the ...\n",
       "11648    cons 1  poor story 2  too many sjw 3  don t in...\n",
       "11662    my heart is broken when i see game industry in...\n",
       "                               ...                        \n",
       "34489    it is just a big joke neil druckman is idiot s...\n",
       "34497    another sjw game  doesn t worth any money  don...\n",
       "34524                anita sjw                         mc \n",
       "34533    technically it s a masterpiece but the story i...\n",
       "34559    10     10          0                   10     ...\n",
       "Name: review, Length: 996, dtype: object"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_bad[reviews_bad.review.str.contains('sjw')]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
